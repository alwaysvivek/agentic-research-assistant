{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fcff233",
   "metadata": {},
   "source": [
    "# Introduction to Gemini\n",
    "\n",
    "The Gemini API can be used to generate many different outputs such as text, images, video and audio. In this tutorial, we'll go through text generation and check some metadata and tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8673ad3",
   "metadata": {},
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd5c26ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 funny jokes about data engineering:\n",
      "\n",
      "1.  **A data engineer, a data scientist, and a business analyst walk into a bar.** The data engineer immediately asks, \"Is the tap data clean? And how often do you refresh the ice bucket?\"\n",
      "2.  **What's a data engineer's favorite bedtime story?** The one about the pipeline that ran successfully on the first try, with no schema changes, and then worked perfectly in production for a year.\n",
      "3.  **A business stakeholder asks a data engineer for \"real-time insights.\"** The data engineer smiles and says, \"Great! How real-time are we talking? Is a 24-hour latency considered 'real-enough' time for you?\"\n",
      "4.  **Why did the data engineer get kicked out of the library?** Because they kept trying to enforce a strict schema on the Dewey Decimal System.\n",
      "5.  **My therapist told me to identify my triggers.** I just showed them my Airflow dashboard.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Generate some funny jokes about data engineering. Give 5 points\",\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fdc21ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sdk_http_response', 'candidates', 'create_time', 'model_version', 'prompt_feedback', 'response_id', 'usage_metadata', 'automatic_function_calling_history', 'parsed'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d042c9",
   "metadata": {},
   "source": [
    "### Analyze tokens\n",
    "\n",
    "Tokens are the basic units of text for LLMs used to process and generate language. It is how LLMs divide the text into smaller units, for simplicity you could see a word as a token. Tokens are also what you pay for when you use the APIs\n",
    "\n",
    "The free tier in gemini API allows for (Gemini 2.5 flash)\n",
    "\n",
    "- Requests per minute (RPM): 10\n",
    "- Tokens per minute (TPM): 250 000\n",
    "- Requests per day (RDP): 250\n",
    "\n",
    "as of 2025-11-11\n",
    "\n",
    "[Check documentation here for latest limits](https://ai.google.dev/gemini-api/docs/rate-limits)\n",
    "\n",
    "There is a possiblity to upgrade to higher tiers, which allows for more generous rate limits, but comes with higher costs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9d37685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=262,\n",
       "  prompt_token_count=13,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=13\n",
       "    ),\n",
       "  ],\n",
       "  thoughts_token_count=972,\n",
       "  total_token_count=1247\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = response.usage_metadata\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6926b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "# GenerateContentResponseUsageMetadata is a pydantic model, which means we can\n",
    "# use dot operator to get different attributes\n",
    "isinstance(metadata, BaseModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd5da18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tokens - number of tokens in models response\n",
      "metadata.candidates_token_count = 262\n"
     ]
    }
   ],
   "source": [
    "print(\"Output tokens - number of tokens in models response\")\n",
    "print(f\"{metadata.candidates_token_count = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4a6e7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in user input\n",
      "metadata.prompt_token_count = 13\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens in user input\")\n",
    "print(f\"{metadata.prompt_token_count = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0de3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens used for internal thinking\n",
      "metadata.thoughts_token_count = 972\n"
     ]
    }
   ],
   "source": [
    "# a lot of tokens used here\n",
    "print(\"Tokens used for internal thinking\")\n",
    "print(f\"{metadata.thoughts_token_count = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ca0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens used for internal thinking\n",
      "metadata.total_token_count = 1247\n"
     ]
    }
   ],
   "source": [
    "print(\"Total tokens used - this is the billing number\")\n",
    "print(f\"{metadata.total_token_count = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8f9088",
   "metadata": {},
   "source": [
    "### Thinking \n",
    "\n",
    "- we can see that the thinking process takes a lot of tokens and also takes long time, so if you want answers quicker but chooses to have less thinking you can set the `thinking budget`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c390fd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 funny jokes about data engineering:\n",
      "\n",
      "1. **Why did the data engineer break up with their partner?** Because they kept bringing up old, unstructured data from their past, and the engineer just couldn't process it anymore.\n",
      "\n",
      "2. **What's a data engineer's favorite type of music?** Anything with a good *flow*... especially if it's orchestrated.\n",
      "\n",
      "3. **My doctor told me I needed to reduce my stress, so I became a data engineer.** Now, instead of worrying about my own problems, I worry about data pipelines failing at 3 AM. Much better!\n",
      "\n",
      "4. **A data engineer walks into a bar.** The bartender asks, \"What can I get for you?\" The engineer replies, \"Just give me all your raw ingredients, and I'll figure out how to make a beer that actually works tomorrow morning... maybe.\"\n",
      "\n",
      "5. **You know you're a data engineer when...** you see a beautifully organized spreadsheet and your first thought isn't \"Wow,\" but \"I bet I could automate that into a robust, scalable ELT process.\"\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Generate some funny jokes about data engineering. Give 5 points\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    ),\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "574e1da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerateContentResponseUsageMetadata(\n",
      "  candidates_token_count=233,\n",
      "  prompt_token_count=13,\n",
      "  prompt_tokens_details=[\n",
      "    ModalityTokenCount(\n",
      "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "      token_count=13\n",
      "    ),\n",
      "  ],\n",
      "  total_token_count=246\n",
      ")\n",
      "Ah much cheaper, but is the result as good as the thinking?\n"
     ]
    }
   ],
   "source": [
    "print(repr(response.usage_metadata))\n",
    "\n",
    "print(\"Ah much cheaper, but is the result as good as the thinking?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9642b2e6",
   "metadata": {},
   "source": [
    "## System instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ca63ab5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ERROR 404: Weather data not found! It seems my forecast module is encountering a few bugs and keeps returning a 'Segmentation Fault: Core Dump' every time I try to debug the precipitation levels. Might need a patch!\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_instruction = \"\"\"You are a joking robot called Ro BÃ¥t, which \n",
    "        will always answer with a programming joke.\n",
    "        \"\"\"\n",
    "\n",
    "prompt = \"What is the weather today?\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "    ),\n",
    "    contents=prompt,\n",
    ")\n",
    "\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1cd2ab84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=47,\n",
       "  prompt_token_count=32,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=32\n",
       "    ),\n",
       "  ],\n",
       "  thoughts_token_count=688,\n",
       "  total_token_count=767\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9a1a5c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(system_instruction.split()) = 16\n",
      "len(prompt.split()) = 5\n",
      "plus some formatting overhead\n",
      "prompt token count 32\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(system_instruction.split()) = }\")\n",
    "print(f\"{len(prompt.split()) = }\")\n",
    "print(\"plus some formatting overhead\")\n",
    "\n",
    "print(f\"prompt token count {response.usage_metadata.prompt_token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e369e709",
   "metadata": {},
   "source": [
    "### Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "06b13e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A small gray rabbit, its fur the color of twilight, twitched its nose, sampling the scent of fresh clover. With a flick of its ears, it hopped deeper into the meadow, a tiny shadow against the rising sun.\n"
     ]
    }
   ],
   "source": [
    "story_prompt = \"write a 2 sentence story about a gray rabbit\"\n",
    "\n",
    "boring_story = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=story_prompt,\n",
    "    config=types.GenerateContentConfig(temperature=0.0),\n",
    ")\n",
    "\n",
    "print(boring_story.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed8991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A small gray rabbit, its fur the color of a stormy sky, twitched its nose, sniffing the damp morning air for tender shoots. With a sudden flick of its ears, it vanished into the dense undergrowth, a silent gray blur against the green.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# you can see that the outputs are similar to the first when temperature is 0\n",
    "boring_story = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=story_prompt,\n",
    "    config=types.GenerateContentConfig(temperature=0.0),\n",
    ")\n",
    "\n",
    "print(boring_story.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7a85d355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dusty, a small gray rabbit, sat absolutely still in the tall grass, his twitching nose the only sign of life. A loud snap of a twig nearby sent him bolting like a silver blur straight into the safety of his burrow.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "creative_story = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=story_prompt,\n",
    "    config=types.GenerateContentConfig(temperature=2.0),\n",
    ")\n",
    "\n",
    "print(creative_story.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f581ae8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A small, gray rabbit cautiously emerged from its burrow, twitching its nose at the fresh morning dew. It hopped quietly across the lawn, ever watchful for hawks, until it found a patch of juicy dandelions to nibble.\n"
     ]
    }
   ],
   "source": [
    "# very different story\n",
    "creative_story = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=story_prompt,\n",
    "    config=types.GenerateContentConfig(temperature=2.0),\n",
    ")\n",
    "\n",
    "print(creative_story.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e552757d",
   "metadata": {},
   "source": [
    "## Multimodal inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a657284c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the image, here's a description of this \"dude\":\n",
      "\n",
      "**Appearance:**\n",
      "*   He is a young adult male, likely in his 20s or early 30s, depicted in a cartoon style.\n",
      "*   He has short, dark, slightly spiky hair.\n",
      "*   He wears rectangular, black-rimmed glasses, which often suggest a studious or intellectual personality.\n",
      "*   His skin tone is light to medium.\n",
      "*   He has a neutral to slightly serious or contemplative expression on his face, with a straight mouth and slightly furrowed brows, suggesting focus or deep thought.\n",
      "\n",
      "**Clothing:**\n",
      "*   He is wearing a raglan-style shirt (also known as a baseball tee) with black sleeves and an off-white/tan body.\n",
      "*   The most defining feature is his shirt graphic, which displays a humorous statistical pun:\n",
      "    *   Above a classic bell curve (representing a normal distribution), it reads \"NORMAL DISTRIBUTION.\"\n",
      "    *   Below, above a ghost-shaped curve (which also somewhat resembles a bell curve), it reads \"PARANORMAL DISTRIBUTION.\"\n",
      "\n",
      "**Activity and Setting:**\n",
      "*   He is shown sitting at a table, actively typing on a silver laptop with a visible Apple logo.\n",
      "*   His hands are positioned over the keyboard, indicating engagement with his work or studies.\n",
      "*   The background is a plain, dark color, keeping the focus entirely on him and his activity.\n",
      "\n",
      "**Inferred Personality/Interests:**\n",
      "*   **Intellectual/Analytical:** The shirt's graphic strongly suggests an interest in statistics, mathematics, data science, or a related analytical field. He likely works with data or is a student in a quantitative discipline.\n",
      "*   **Humorous (Dry Wit):** The \"paranormal distribution\" pun indicates a dry, clever, and somewhat \"nerdy\" sense of humor. He probably appreciates wordplay and intellectual jokes.\n",
      "*   **Focused/Diligent:** His expression and posture suggest he is currently engrossed and dedicated to whatever he's doing on his laptop.\n",
      "*   **Tech-savvy:** The presence of a modern laptop, specifically an Apple one, points to someone comfortable and reliant on technology.\n",
      "\n",
      "In summary, this dude appears to be a smart, focused, and slightly quirky individual with a penchant for statistics and a good sense of humor, likely engrossed in some kind of analytical or technical work.\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents={\n",
    "        \"parts\": [\n",
    "            {\"text\": \"Tell me about this dude\"},\n",
    "            {\n",
    "                \"inline_data\": {\n",
    "                    \"mime_type\": \"image/png\",\n",
    "                    \"data\": open(\"assets/kokchun.png\", \"rb\").read(),\n",
    "                }\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68a2b40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-four-weeks-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
