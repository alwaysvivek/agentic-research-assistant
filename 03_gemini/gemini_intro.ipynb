{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fcff233",
   "metadata": {},
   "source": [
    "# Introduction to Gemini\n",
    "\n",
    "The Gemini API can be used to generate many different outputs such as text, images, video and audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8673ad3",
   "metadata": {},
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd5c26ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 funny jokes about data engineering:\n",
      "\n",
      "1.  **Why did the data engineer get kicked out of the orchestra?**\n",
      "    He kept yelling, \"Your pipes are broken, and the whole show is failing!\"\n",
      "\n",
      "2.  **A data engineer's favorite bedtime story starts with...**\n",
      "    \"Once upon a time, in a distributed system far, far away, a single byte went missing, and the entire downstream report was off by 0.0001%...\"\n",
      "\n",
      "3.  **My therapist told me I need to confront the things that scare me.**\n",
      "    So, I walked into the office, stared at our production database's `ALTER TABLE` statement, and screamed.\n",
      "\n",
      "4.  **What's a data engineer's worst nightmare?**\n",
      "    Being told the \"data lake\" is actually just a folder on someone's desktop named \"misc\\_stuff\\_final\\_final\\_v2.zip\".\n",
      "\n",
      "5.  **How many data engineers does it take to change a lightbulb?**\n",
      "    None. They'll build an automated pipeline to detect darkness, trigger a JIRA ticket for the facilities team, and then spend three days debugging why the sensor reported 'null' instead of 'dark'.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Generate some funny jokes about data engineering. Give 5 points\",\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fdc21ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sdk_http_response', 'candidates', 'create_time', 'model_version', 'prompt_feedback', 'response_id', 'usage_metadata', 'automatic_function_calling_history', 'parsed'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d042c9",
   "metadata": {},
   "source": [
    "### Analyze tokens\n",
    "\n",
    "Tokens are the basic units of text for LLMs used to process and generate language. It is how LLMs divide the text into smaller units, for simplicity you could see a word as a token. Tokens are also what you pay for when you use the APIs\n",
    "\n",
    "The free tier in gemini API allows for (Gemini 2.5 flash)\n",
    "\n",
    "- Requests per minute (RPM): 10\n",
    "- Tokens per minute (TPM): 250 000\n",
    "- Requests per day (RDP): 250\n",
    "\n",
    "as of 2025-11-11\n",
    "\n",
    "[Check documentation here for latest limits](https://ai.google.dev/gemini-api/docs/rate-limits)\n",
    "\n",
    "There is a possiblity to upgrade to higher tiers, which allows for more generous rate limits, but comes with higher costs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9d37685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=262,\n",
       "  prompt_token_count=13,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=13\n",
       "    ),\n",
       "  ],\n",
       "  thoughts_token_count=972,\n",
       "  total_token_count=1247\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = response.usage_metadata\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6926b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "# GenerateContentResponseUsageMetadata is a pydantic model, which means we can\n",
    "# use dot operator to get different attributes\n",
    "isinstance(metadata, BaseModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd5da18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tokens - number of tokens in models response\n",
      "metadata.candidates_token_count = 262\n"
     ]
    }
   ],
   "source": [
    "print(\"Output tokens - number of tokens in models response\")\n",
    "print(f\"{metadata.candidates_token_count = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4a6e7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in user input\n",
      "metadata.prompt_token_count = 13\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens in user input\")\n",
    "print(f\"{metadata.prompt_token_count = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0de3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens used for internal thinking\n",
      "metadata.thoughts_token_count = 972\n"
     ]
    }
   ],
   "source": [
    "# a lot of tokens used here\n",
    "print(\"Tokens used for internal thinking\")\n",
    "print(f\"{metadata.thoughts_token_count = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ca0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens used for internal thinking\n",
      "metadata.total_token_count = 1247\n"
     ]
    }
   ],
   "source": [
    "print(\"Total tokens used - this is the billing number\")\n",
    "print(f\"{metadata.total_token_count = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8f9088",
   "metadata": {},
   "source": [
    "### Thinking \n",
    "\n",
    "- we can see that the thinking process takes a lot of tokens and also takes long time, so if you want answers quicker but chooses to have less thinking you can set the `thinking budget`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c390fd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 funny jokes about data engineering:\n",
      "\n",
      "1. **Why did the data engineer break up with their partner?** Because they kept bringing up old, unstructured data from their past, and the engineer just couldn't process it anymore.\n",
      "\n",
      "2. **What's a data engineer's favorite type of music?** Anything with a good *flow*... especially if it's orchestrated.\n",
      "\n",
      "3. **My doctor told me I needed to reduce my stress, so I became a data engineer.** Now, instead of worrying about my own problems, I worry about data pipelines failing at 3 AM. Much better!\n",
      "\n",
      "4. **A data engineer walks into a bar.** The bartender asks, \"What can I get for you?\" The engineer replies, \"Just give me all your raw ingredients, and I'll figure out how to make a beer that actually works tomorrow morning... maybe.\"\n",
      "\n",
      "5. **You know you're a data engineer when...** you see a beautifully organized spreadsheet and your first thought isn't \"Wow,\" but \"I bet I could automate that into a robust, scalable ELT process.\"\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Generate some funny jokes about data engineering. Give 5 points\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    ),\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "574e1da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerateContentResponseUsageMetadata(\n",
      "  candidates_token_count=233,\n",
      "  prompt_token_count=13,\n",
      "  prompt_tokens_details=[\n",
      "    ModalityTokenCount(\n",
      "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "      token_count=13\n",
      "    ),\n",
      "  ],\n",
      "  total_token_count=246\n",
      ")\n",
      "Ah much cheaper, but is the result as good as the thinking?\n"
     ]
    }
   ],
   "source": [
    "print(repr(response.usage_metadata))\n",
    "\n",
    "print(\"Ah much cheaper, but is the result as good as the thinking?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9642b2e6",
   "metadata": {},
   "source": [
    "## System instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca63ab5a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-four-weeks-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
